{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.layers import MaxPooling2D, UpSampling2D\n",
        "from keras.layers import Conv2D, Input, BatchNormalization\n",
        "from keras.layers import Flatten, Dense, Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import argparse\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "import platform\n",
        "\n",
        "from keras.models import load_model\n",
        "import math\n",
        "\n",
        "# def encode_and_connect(input_img, filters):\n",
        "#     #input 28 x 28 x 1\n",
        "\n",
        "#     conv1 = Conv2D(filters, (3,3), activation='relu', padding='same')(input_img) # 28 x 28 x 32\n",
        "#     conv1 = BatchNormalization()(conv1)\n",
        "#     conv1 = Conv2D(filters, (3,3), activation='relu', padding='same')(conv1)\n",
        "#     conv1 = BatchNormalization()(conv1)\n",
        "#     pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) # 14 x 14 x 32\n",
        "#     # pool1 = Dropout(0.52)(pool1)\n",
        "#     filters=filters*2\n",
        "#     conv2 = Conv2D(filters, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
        "#     conv2 = BatchNormalization()(conv2)\n",
        "#     conv2 = Conv2D(filters, (3, 3), activation='relu', padding='same')(conv2)\n",
        "#     conv2 = BatchNormalization()(conv2)\n",
        "#     pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
        "#     # pool2 = Dropout(0.52)(pool2)\n",
        "#     filters=filters*2\n",
        "#     conv3 = Conv2D(filters, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small & thick)\n",
        "#     conv3 = BatchNormalization()(conv3)\n",
        "#     conv3 = Conv2D(filters, (3, 3), activation='relu', padding='same')(conv3)\n",
        "#     conv3 = BatchNormalization()(conv3)\n",
        "#     filters=filters*2\n",
        "#     conv4 = Conv2D(filters, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small & thick)\n",
        "#     conv4 = BatchNormalization()(conv4)\n",
        "#     conv4 = Conv2D(filters, (3, 3), activation='relu', padding='same')(conv4)\n",
        "#     conv4 = BatchNormalization()(conv4)\n",
        "#     temp = Flatten()(conv4)\n",
        "#     dence = Dense(128, activation='relu')(temp)\n",
        "#     layers = Dense(10, activation='softmax')(dence)\n",
        "    \n",
        "#     # drop1 = Dropout(0.25)(temp)\n",
        "#     # dence = Dense(128, activation='relu')(drop1)\n",
        "#     # drop2 = Dropout(0.3)(dence)\n",
        "#     # layers = Dense(10, activation='softmax')(drop2)\n",
        "#     return layers\n",
        "\n",
        "def encoder(input_img, filters):\n",
        "    #encoder\n",
        "    #input 28 x 28 x 1\n",
        "\n",
        "    conv1 = Conv2D(filters, (3,3), activation='relu', padding='same')(input_img) # 28 x 28 x 32\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(filters, (3,3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) # 14 x 14 x 32\n",
        "    pool1 = Dropout(0.40)(pool1)    #first dropout\n",
        "    filters=filters*2\n",
        "    conv2 = Conv2D(filters, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(filters, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
        "    pool2 = Dropout(0.30)(pool2)    #second dropout\n",
        "    filters=filters*2\n",
        "    conv3 = Conv2D(filters, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small & thick)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(filters, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    filters=filters*2\n",
        "    #conv3 = Dropout(0.30)(conv3)    #drop3\n",
        "    conv4 = Conv2D(filters, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small & thick)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(filters, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    return conv4\n",
        "\n",
        "def fully_connected(encode, filters):\n",
        "    temp = Flatten()(encode)\n",
        "    dence = Dense(128, activation='relu')(temp)\n",
        "    dence = Dropout(0.40)(dence)\n",
        "    layers = Dense(10, activation='softmax')(dence)\n",
        "    return layers\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # reading the mnist files\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--train_set\", \"-d\", type=str, required=True)\n",
        "    parser.add_argument(\"--train_labels\", \"-dl\", type=str, required=True)\n",
        "    parser.add_argument(\"--test_set\", \"-t\", type=str, required=True)\n",
        "    parser.add_argument(\"--test_labels\", \"-tl\", type=str, required=True)\n",
        "    parser.add_argument(\"--model\", \"-model\", type=str, required=True)\n",
        "    args = parser.parse_args()\n",
        "    print(args.train_set)\n",
        "    print(args.train_labels)\n",
        "\n",
        "\n",
        "    X,Y = loadlocal_mnist(\n",
        "        images_path=args.train_set,\n",
        "        labels_path=args.train_labels)\n",
        "        \n",
        "    number_of_images_train = int(X.shape[0])\n",
        "    number_of_images_train = 5000\n",
        "    dimensions = int(X.shape[1])\n",
        "\n",
        "    X1,Y1 = loadlocal_mnist(\n",
        "        images_path=args.test_set,\n",
        "        labels_path=args.test_labels)\n",
        "        \n",
        "    number_of_images_test = int(X1.shape[0])\n",
        "    number_of_images_test = 850\n",
        "\n",
        "\n",
        "    print('Dimensions: %s x %s' % (X.shape[0],X.shape[1]))\n",
        "    print('Digits:  0 1 2 3 4 5 6 7 8 9')\n",
        "    print('labels: %s' % np.unique(Y))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # model=Model.create_model()\n",
        "    # model.evaluate(test_X, y1)\n",
        "    # model.load_weights(args.model)\n",
        "\n",
        "    model=load_model(args.model)        #mhpws to valoume mesa sth while kai check gia alla batches\n",
        "    weights=model.get_weights()\n",
        "#    model.summary()\n",
        "\n",
        "    history_list = []\n",
        "    prediction_list = []\n",
        "    while(1):\n",
        "        x, y = int(math.sqrt(dimensions)), int(math.sqrt(dimensions))\n",
        "        inChannel =  input(\"inChannel: \")\n",
        "        batch_size = input(\"Batch Size: \") #128 stis diafaneies\n",
        "        epochsenc = input(\"Epochs for fully connected part: \")\n",
        "        epochs = input(\"Epochs for whole model: \")\n",
        "        inChannel = int(inChannel)\n",
        "        batch_size = int(batch_size)\n",
        "        epochs = int(epochs)\n",
        "        epochsenc = int(epochsenc)\n",
        "        filters = input(\"Give me the number of filters: \")\n",
        "        filters = int(filters)\n",
        "        input_img =  Input(shape=(x, y, inChannel), name='input')\n",
        "\n",
        "        train_X = np.reshape(X, (len(X), 28, 28, 1))\n",
        "        train_X = train_X.astype('float32')\n",
        "        train_X = train_X/255.0\n",
        "\n",
        "        train_X = train_X[:number_of_images_train]\n",
        "        Y = Y[:number_of_images_train]\n",
        "\n",
        "        test_X = np.reshape(X1, (len(X1), 28, 28, 1))\n",
        "        test_X = test_X.astype('float32')\n",
        "        test_X = test_X/255.0\n",
        "\n",
        "        test_X = test_X[:number_of_images_test]\n",
        "        Y1 = Y1[:number_of_images_test]\n",
        "\n",
        "        train_Y_one_hot = to_categorical(Y)\n",
        "        test_Y_one_hot = to_categorical(Y1)\n",
        "\n",
        "        encode = encoder(input_img, filters)\n",
        "        fc_model = Model(input_img, fully_connected(encode, filters))\n",
        "#        fc_model = Model(input_img, encode_and_connect(input_img, filters))\n",
        "        for m1, m2 in zip(fc_model.layers[:21], model.layers[0:21]):\n",
        "            m1.set_weights(m2.get_weights())\n",
        "\n",
        "        #train only encode (all layers after 20 layers of encode since its pretrained)\n",
        "        for x in fc_model.layers[0:21]:\n",
        "            x.trainable=False\n",
        "        fc_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.RMSprop(),metrics=['accuracy'])\n",
        "        train_X,valid_X,train_label,valid_label = train_test_split(train_X,train_Y_one_hot,test_size=0.20,random_state=13)   #splitting persentage can change\n",
        "        fc_train = fc_model.fit(train_X, train_label, batch_size=batch_size ,epochs=epochsenc,verbose=1,validation_data=(valid_X, valid_label))\n",
        "\n",
        "        #train whole model (all layers including already trained)\n",
        "        for x in fc_model.layers[:21]:\n",
        "            x.trainable=True\n",
        "        fc_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.RMSprop(),metrics=['accuracy'])\n",
        "        fc_train = fc_model.fit(train_X, train_label, batch_size=batch_size ,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
        "\n",
        "        history_list.append((fc_train,batch_size,inChannel,epochs,filters))\n",
        "        predicted_classes = fc_model.predict(test_X)\n",
        "        predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "        test_eval = fc_model.evaluate(test_X, test_Y_one_hot, verbose=0)\n",
        "\n",
        "\n",
        "        prediction_list.append((predicted_classes,test_eval))\n",
        "\n",
        "        pl = input(\"Type 'yes' to plot: \")\n",
        "        if(pl == 'yes'):\n",
        "            for history,predict in zip(history_list,prediction_list):\n",
        "                print('Batches: %d\\ninChannell: %d\\nEpochs: %d\\nFilters: %d' %(history[1], history[2], history[3], history[4]))\n",
        "                target_names = [\"Number {}\".format(i) for i in range(10)]\n",
        "                print('Test loss:', predict[1][0])\n",
        "                print('Test accuracy:', predict[1][1])\n",
        "                print(classification_report(Y1, predict[0], target_names=target_names))\n",
        "                accuracy = history[0].history['accuracy']\n",
        "                val_accuracy = history[0].history['val_accuracy']\n",
        "                loss = history[0].history['loss']\n",
        "                val_loss = history[0].history['val_loss']\n",
        "                epochs = range(len(accuracy))\n",
        "                plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
        "                plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "                plt.title('Training and validation accuracy')\n",
        "                plt.title('Batches: %d\\ninChannell: %d\\nEpochs: %d\\nFilters: %d' %(history[1], history[2], history[3], history[4]), loc='left')\n",
        "                plt.xlabel('epochs')\n",
        "                plt.legend()\n",
        "                plt.figure()\n",
        "                plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "                plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "                plt.title('Training and validation loss')\n",
        "                plt.title('Batches: %d\\ninChannell: %d\\nEpochs: %d\\nFilters: %d' %(history[1], history[2], history[3], history[4]), loc='left')\n",
        "                plt.xlabel('epochs')\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "\n",
        "        next_move = input(\"Type 'print' to print: \")\n",
        "        if(next_move == 'print'):\n",
        "            # test_eval = fc_model.evaluate(test_X, test_Y_one_hot, verbose=0)\n",
        "            # print('Test loss:', test_eval[0])\n",
        "            # print('Test accuracy:', test_eval[1])\n",
        "            flag = False\n",
        "            while(flag == False):\n",
        "                uinChannel =  input(\"inChannel: \")\n",
        "                ubatch_size = input(\"Batch Size: \") #128 stis diafaneies\n",
        "                uepochsenc = input(\"Epochs for fully connected part: \")\n",
        "                uepochs = input(\"Epochs for whole model: \")\n",
        "                uinChannel = int(uinChannel)\n",
        "                ubatch_size = int(ubatch_size)\n",
        "                uepochs = int(uepochs)\n",
        "                uepochsenc = int(uepochsenc)\n",
        "                ufilters = input(\"Give me the number of filters: \")\n",
        "                ufilters = int(ufilters)\n",
        "                #flag = False\n",
        "                for history,predict in zip(history_list,prediction_list):\n",
        "                    if(history[1] == ubatch_size and history[2] == uinChannel and history[3] == uepochs and history[4] == ufilters):\n",
        "                        # predicted_classes = fc_model.predict(test_X)\n",
        "                        # predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "                        #predicted_classes.shape, test_labels.shape\n",
        "                        correct = np.where(predict[0]==Y1)[0]\n",
        "                        print (\"Found %d correct labels\" % len(correct))\n",
        "                        incorrect = np.where(predict[0] !=Y1)[0]\n",
        "                        print (\"Found %d incorrect labels\" % len(incorrect))\n",
        "\n",
        "                        for i, num in enumerate(test_X[:25]):\n",
        "                            plt.subplot(5,5,i+1)\n",
        "                            plt.imshow(test_X[i].reshape(28,28), cmap='gray', interpolation='none')\n",
        "                            plt.title(\"Predicted {}, Label {}\".format(predict[0][i], Y1[i]))\n",
        "                            plt.tight_layout()\n",
        "                        plt.show()\n",
        "                        flag = True\n",
        "                if(flag == False):\n",
        "                    print(\"Not found\")\n",
        "                    answer = input(\"You want to try again? Type 'yes': \")\n",
        "                    if(answer != 'yes'):\n",
        "                        break\n",
        "\n",
        "            # target_names = [\"Number {}\".format(i) for i in range(10)]\n",
        "            # print(classification_report(Y1, predicted_classes, target_names=target_names))\n",
        "\n",
        "        next_move = input(\"Type 'save' to save: \")\n",
        "        if(next_move == 'save'):\n",
        "            path = input(\"Give me the path to save the previous fully connected model: \")\n",
        "            fc_model.save(path)\n",
        "\n",
        "        next_move = input(\"Type '0' to stop: \")\n",
        "        if(next_move == '0'):\n",
        "            exit()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}